{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NP_Xq8p7WubK"
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> <b> Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: \n",
    "\n",
    "The goal is to implement paragraph vector models for text representation by preparing the data and leveraging two key approaches: Distributed Bag of Words (DBoW) and Distributed Memory (PV-DM). These methods capture semantic information to create meaningful paragraph embeddings for downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL-ow34KEKNG"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "- Demonstration of Doc2Vec using a custom corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsgTb3r2EKNH"
   },
   "source": [
    "# <a id= 'dv0'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Table of contents:**<br>\n",
    "[1. Install and import the requirements](#dv1)<br>\n",
    "[2. Preparing the data](#dv2)<br>\n",
    "[3. Distributed bag of words version of paragraph vector (DBoW)](#dv3)<br>\n",
    "[4. Distributed memory version of paragraph vector (PV-DM)](#dv4)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEHv6ada5Sau"
   },
   "source": [
    "##### <a id = 'dv1'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Install and import the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Re8l4wPa85tb",
    "outputId": "38d9a53e-f197-4f97-c48e-326ee5379092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.2.0\n",
    "!pip install nltk==3.8.1\n",
    "!pip install spacy==3.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePx4pE-N6QfR"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jifk_HHmvVWf"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PZSMIYIr7C0f"
   },
   "outputs": [],
   "source": [
    "# To suppress warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1dMIBJz7PHZ"
   },
   "source": [
    "<font size = 5 color = seagreen><b> Download the necessary corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PuxN4-RL64Dp",
    "outputId": "e64536d3-ca4d-4d2c-f260-d7d7e397f810"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWR4uZMQEKNL"
   },
   "source": [
    "[top](#dv0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tyMjQ37EKNL"
   },
   "source": [
    "##### <a id = 'dv2'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW6QryoI7dmC"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b> Define the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_hg1ZpYcJVez"
   },
   "outputs": [],
   "source": [
    "documents = [\"The analyst reviews the dataset to identify trends and patterns.\"\n",
    "             \"Data analysis helps businesses make informed decisions based on facts and figures.\",\n",
    "             \"In a research project the team gathers data for subsequent analysis.\",\n",
    "             \"Charts and graphs are used to visually represent the results of data analysis.\",\n",
    "             \"Analyzing customer feedback data provides valuable insights for product improvement.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVoMDTcA-Ap9"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>Create tagged documents:\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "    \n",
    "- The TaggedDocument function represents document along with a tag.\n",
    "- This generates data in the acceptable input format for the Doc2Vec function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "343BsPaV7rvu"
   },
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(word.lower()), tags=[str(i)]) for i, word in enumerate(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPssbTNiwEd9",
    "outputId": "e9528dc8-d6e2-4f74-f97e-1758e7d8be99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['in', 'a', 'research', 'project', 'the', 'team', 'gathers', 'data', 'for', 'subsequent', 'analysis', '.'], ['1']>\n"
     ]
    }
   ],
   "source": [
    "print(tagged_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ge09cDvNEKNM"
   },
   "source": [
    "[top](#dv0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsZMArB59BZA"
   },
   "source": [
    "##### <a id = 'dv3'>\n",
    "<font size = 10 color = 'midnightblue'> <b> Distributed bag of words version of paragraph vector (DBoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd7sjC4yEKNM"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "    \n",
    "- The model is trained to predict words randomly sampled from the paragraph (document) it is processing, without using the word order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stbu-okACpCS"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Create the model object with tagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dcxU67TCwSd8"
   },
   "outputs": [],
   "source": [
    "dbow_model = Doc2Vec(tagged_data,vector_size=20, min_count=1, epochs=2,dm=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XLVOIlKD7ql"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Get feature vector for : \"***Data analysis identifies trends and patterns.***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLkpnvcTx6T9",
    "outputId": "b3a456d3-363f-41ab-e080-218aed48fe77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02037955 -0.01731416 -0.01322459 -0.01250527  0.01428972 -0.01562179\n",
      " -0.00683926 -0.0102631   0.00712917  0.01492114  0.01172474 -0.00732228\n",
      " -0.02340268  0.01833704 -0.00616148  0.011989    0.01559411  0.02362764\n",
      " -0.00038035  0.0149787 ]\n"
     ]
    }
   ],
   "source": [
    "print(dbow_model.infer_vector([\"Data\", \"analysis\", \"identifies\", \"trends\", \"and\", \"patterns\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9uY0_1lFWzT"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Get top 5 most simlar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KzwAgUJzQLW",
    "outputId": "9f9a210d-d674-4b1d-ccc3-81c38e94ff78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subsequent', 0.5829257965087891),\n",
       " ('reviews', 0.38686245679855347),\n",
       " ('dataset', 0.3179265260696411),\n",
       " ('insights', 0.26625779271125793),\n",
       " ('research', 0.19855976104736328)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbow_model.wv.most_similar(\"analysis\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyWYY8vNLCxT"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Get the cosine similarity between the two sets of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myGVWgudz9mW",
    "outputId": "525f1d56-f255-4421-e532-2acd6ec50f19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20002559"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbow_model.wv.n_similarity([\"data\", \"analysis\"],[\"insights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYAvCHlNEKNO"
   },
   "source": [
    "[top](#dv0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfNHT7mN9Wcu"
   },
   "source": [
    "##### <a id = 'dv4'>\n",
    "<font size = 10 color = 'midnightblue'> <b> Distributed memory version of paragraph vector (PV-DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distributed Memory Model of Paragraph Vectors (PV-DM) is one of the two approaches introduced by Le and Mikolov in their 2014 paper on Paragraph Vector (Doc2Vec) models. PV-DM is a powerful method for generating dense, fixed-length vector representations of texts (like sentences, paragraphs, or entire documents) by leveraging the surrounding context of words, making it ideal for downstream NLP tasks like classification, clustering, or semantic search.\n",
    "\n",
    "## Understanding PV-DM\n",
    "The PV-DM model is conceptually similar to Word2Vec's Continuous Bag of Words (CBOW) approach, with a key enhancement: it takes into account the document's unique identifier (a \"paragraph vector\") alongside the context words when predicting a target word. This helps the model capture both local word semantics and global document-level information.\n",
    "\n",
    "## Key Features of PV-DM\n",
    "- Context-Aware: By leveraging both word context and paragraph vectors, PV-DM captures both local (word-level) and global (document-level) semantics.\n",
    "- Flexible Length: Unlike fixed-length features (e.g., Bag-of-Words), PV-DM can generate embeddings for texts of any length.\n",
    "- Document-Level Memory: The paragraph vector provides a \"memory\" of the document, which helps retain information even if certain words are missing in the context.\n",
    "\n",
    "## Example Use Cases\n",
    "- Document Classification: PV-DM embeddings can be used to classify documents based on their content, such as spam detection or sentiment analysis.\n",
    "- Semantic Similarity: Finding similar documents by comparing paragraph vectors can be useful in search engines or recommendation systems.\n",
    "- Text Clustering: Grouping similar documents together for organizing large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUdamla-L42Z"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Create model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1i2Vv2uY4kqg"
   },
   "outputs": [],
   "source": [
    "dm_model = Doc2Vec(tagged_data, min_count=1, vector_size=20, epochs=2,dm=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_FgdkqXL9cC"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Get feature vector for : \"***Data analysis identifies trends and patterns.***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDO7WopSLtwB",
    "outputId": "b4b7807b-0f7f-4547-a30b-9cf5b63551bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02037944 -0.01731404 -0.01322444 -0.01250534  0.0142896  -0.01562171\n",
      " -0.00683909 -0.01026298  0.00712932  0.01492122  0.01172474 -0.00732237\n",
      " -0.02340247  0.01833722 -0.00616135  0.01198908  0.01559412  0.02362749\n",
      " -0.00038012  0.01497868]\n"
     ]
    }
   ],
   "source": [
    "print(dm_model.infer_vector([\"Data\", \"analysis\", \"identifies\", \"trends\", \"and\", \"patterns\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGz8JqmzMfbf"
   },
   "source": [
    "<font size = 5 color = pwdrblue><b>  Get top5 most similar keys to given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8EGESwwMXdc",
    "outputId": "e58d4111-223e-40a5-92e5-2aaeac6f6386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('subsequent', 0.5829257965087891),\n",
       " ('reviews', 0.386778861284256),\n",
       " ('dataset', 0.31784698367118835),\n",
       " ('insights', 0.2662583887577057),\n",
       " ('research', 0.19845950603485107)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_model.wv.most_similar(\"analysis\",topn=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0g2uTeNMYF1",
    "outputId": "0426cc37-995a-4afd-f26b-5fdfa5f4a85d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20011897"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_model.wv.n_similarity([\"data\", \"analysis\"],[\"insights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmmM7yI31gMn"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "<center> <b> What happens when we compare between words which are not in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YS-mciDx0ZiA",
    "outputId": "4c2f8d8e-b1a9-4c8e-a609-ed82d36e1ad0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_model.wv.n_similarity(['covid'],['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfKvhnEgM-nX"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "    \n",
    "<center> <b>If the word is not in vocabulary the similarity score with other words will be zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOEUgGWyEKNW"
   },
   "source": [
    "[top](#dv0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
