{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176c3b7b-2770-4ee4-8a6c-159402ade70a",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Machine Translation with Encoder-Decoder Model\n",
    "# ============================\n",
    "\n",
    "## Overview: \n",
    "\n",
    "The goal is to implement a machine translation system using an encoder-decoder model. It involves preparing a suitable dataset, defining the model architecture, training it to translate between languages, and performing inference to generate translations. The conclusion summarizes the results and insights from the implementation.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Dataset Preparation](#dataset-preparation)\n",
    "3. [Model Architecture](#model-architecture)\n",
    "4. [Training the Model](#training-the-model)\n",
    "5. [Inference](#inference)\n",
    "6. [Conclusion](#conclusion)\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will implement an Encoder-Decoder model for machine translation. The Encoder-Decoder architecture is widely used for tasks involving sequential data, such as language translation. This architecture consists of two main components: an encoder that processes the input sequence and a decoder that generates the output sequence. We will train our model on a simple dataset and then demonstrate how to make predictions for new input sentences.\n",
    "\n",
    "## Dataset Preparation\n",
    "For this example, we will use a simple dataset that consists of English sentences and their corresponding translations in another language (e.g., French).\n",
    "\n",
    "1. **Load the Dataset**: We will load the translation dataset which contains pairs of sentences in English and French.\n",
    "2. **Preprocess the Data**: We will tokenize the sentences, convert words to integers, and pad the sequences to ensure they have a uniform length, which is essential for batch processing in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1395e71a-82a1-47cd-a84b-f41ffca40431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install below libraries if not done\n",
    "!pip install numpy==1.23.5\n",
    "!pip install tensorflow==2.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f78eeac0-040b-49d7-afb6-eadbce443fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2839\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2725\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2608\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2482\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2338\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2171\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1968\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1717\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1404\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1025\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0632\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.0410\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0412\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0524\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0698\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0923\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.1096\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1114\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0957\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0669\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0313\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9933\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9537\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9126\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.8732\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8396\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8172\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8109\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7901\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7548\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.7302\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7130\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6910\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6603\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6289\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6087\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5851\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5547\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5317\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5134\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4892\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4615\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4380\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4165\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3963\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3812\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3650\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3458\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3306\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3177\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3072\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2983\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2847\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2719\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2630\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2584\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2541\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2456\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2397\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2356\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2316\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2226\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2169\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2156\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2118\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2042\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2019\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2023\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1959\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1922\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1909\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1833\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1831\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1841\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1789\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1810\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1754\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1735\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1720\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1660\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1694\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1637\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1686\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1628\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1628\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1614\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1573\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1593\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1542\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1560\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1553\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1527\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1542\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1493\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1510\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1476\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1468\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1472\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1445\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1460\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "1/1 [==============================] - 0s 273ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Decoded sentence: bonjour <end>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Provided data\n",
    "data = {\n",
    "    'english': ['<start> Hello <end>', '<start> How are you? <end>', '<start> I am learning <end>', '<start> Machine Translation is fun <end>'],\n",
    "    'french': ['<start> Bonjour <end>', '<start> Comment ça va? <end>', '<start> J\\'apprends <end>', '<start> La traduction automatique est amusante <end>']\n",
    "}\n",
    "\n",
    "# Tokenization\n",
    "tokenizer_en = Tokenizer(filters=\"\")\n",
    "tokenizer_en.fit_on_texts(data['english'])\n",
    "vocab_size_en = len(tokenizer_en.word_index) + 1  # +1 for padding token\n",
    "\n",
    "tokenizer_fr = Tokenizer(filters=\"\")\n",
    "tokenizer_fr.fit_on_texts(data['french'])\n",
    "vocab_size_fr = len(tokenizer_fr.word_index) + 1  # +1 for padding token\n",
    "\n",
    "# Convert sentences to sequences\n",
    "sequences_en = tokenizer_en.texts_to_sequences(data['english'])\n",
    "sequences_fr = tokenizer_fr.texts_to_sequences(data['french'])\n",
    "\n",
    "# Pad sequences\n",
    "max_length_en = max(len(seq) for seq in sequences_en)\n",
    "max_length_fr = max(len(seq) for seq in sequences_fr)\n",
    "\n",
    "padded_en = pad_sequences(sequences_en, maxlen=max_length_en, padding='post')\n",
    "padded_fr = pad_sequences(sequences_fr, maxlen=max_length_fr, padding='post')\n",
    "\n",
    "# Prepare target data for training\n",
    "fr_target_data = np.zeros((len(padded_fr), max_length_fr, vocab_size_fr))  # (num_samples, max_length, vocab_size)\n",
    "\n",
    "for i, seq in enumerate(sequences_fr):\n",
    "    for t in range(len(seq) - 1):\n",
    "        fr_target_data[i, t, seq[t + 1]] = 1.0  # One-hot encoding\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space\n",
    "embedding_dim = 256  # Dimensionality of the embedding layer\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(None,))  # Input shape for encoder\n",
    "encoder_embedding = Embedding(vocab_size_en, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]  # Encoder states\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(None,))  # Input shape for decoder\n",
    "decoder_embedding = Embedding(vocab_size_fr, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(vocab_size_fr, activation='softmax')\n",
    "\n",
    "# Decoder outputs\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)  # Output layer for decoder\n",
    "\n",
    "# Define the training model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Fit the model (using the prepared data)\n",
    "model.fit([padded_en, padded_fr], fr_target_data, batch_size=64, epochs=100)  # Adjust epochs as needed\n",
    "\n",
    "# Create the encoder model for inference\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Create the decoder model for inference\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_inputs_single = Input(shape=(1,))  # Input for a single token\n",
    "decoder_embedding_single = Embedding(vocab_size_fr, embedding_dim)(decoder_inputs_single)\n",
    "decoder_outputs_single, h, c = decoder_lstm(decoder_embedding_single, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "decoder_outputs_single = decoder_dense(decoder_outputs_single)  # Output layer for decoder\n",
    "\n",
    "# Define the decoder model for inference\n",
    "decoder_model = Model([decoder_inputs_single, decoder_state_input_h, decoder_state_input_c], [decoder_outputs_single, h, c])\n",
    "\n",
    "# Decode sequence function\n",
    "def decode_sequence(input_seq, encoder_model, decoder_model, tokenizer_fr, max_length_fr):\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate the initial target sequence (the start character)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer_fr.word_index['<start>']  # Assuming you have a start token\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Run the decoder model to get the next token\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token and convert it to a character\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = tokenizer_fr.index_word.get(sampled_token_index, '')  # Convert token index to word\n",
    "        decoded_sentence += ' ' + sampled_char  # Append the sampled character to the decoded sentence\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if (sampled_char == '<end>' or len(decoded_sentence.split()) > max_length_fr):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence for the next time step\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()  # Return the decoded sentence without leading whitespace\n",
    "\n",
    "# Example usage\n",
    "# Prepare the input sequence (using the first sample as an example)\n",
    "input_seq = padded_en[0].reshape(1, -1)  # Example input\n",
    "input_seq = pad_sequences(input_seq, maxlen=max_length_en, padding='post')  # Pad input sequence\n",
    "\n",
    "decoded_sentence = decode_sequence(input_seq, encoder_model, decoder_model, tokenizer_fr, max_length_fr)\n",
    "print(f'Decoded sentence: {decoded_sentence}')  # Print the decoded sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fc61a-313b-47bf-bccd-76bb3c6742bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99ad13-0b2d-4cff-956e-2e123c5f21d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
