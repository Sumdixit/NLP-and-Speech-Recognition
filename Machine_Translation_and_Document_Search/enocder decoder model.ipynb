{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fe1ad1-8ee5-4ce8-9762-7dcb8fb9fd15",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Encoder-Decoder Architecture for Sentiment Analysis\n",
    "# ============================\n",
    "\n",
    "## Overview:\n",
    "\n",
    "The goal is to implement an encoder-decoder architecture for sentiment analysis using the IMDB dataset. The process includes importing libraries, preprocessing data, defining the model, preparing decoder inputs, fitting the model, and evaluating its performance. The conclusion summarizes the effectiveness of this architecture in sentiment analysis.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Importing Necessary Libraries](#importing-necessary-libraries)\n",
    "3. [Loading and Preprocessing the IMDB Dataset](#loading-and-preprocessing-the-imdb-dataset)\n",
    "4. [Defining the Encoder-Decoder Model](#defining-the-encoder-decoder-model)\n",
    "5. [Preparing Decoder Input Data](#preparing-decoder-input-data)\n",
    "6. [Fitting the Model](#fitting-the-model)\n",
    "7. [Evaluating the Model](#evaluating-the-model)\n",
    "8. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0b4f9-bb6f-4994-848f-fe8c65a1d691",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Encoder-decoder architectures are widely used in Natural Language Processing (NLP) tasks such as machine translation, text summarization, and sentiment analysis. This architecture consists of two main components:\n",
    "\n",
    "- **Encoder**: The encoder processes the input data (in our case, movie reviews) and compresses it into a fixed-size context vector. This vector summarizes the semantic meaning of the input.\n",
    "  \n",
    "- **Decoder**: The decoder uses the context vector to generate an output sequence (e.g., predicting sentiment). It can produce output one token at a time, incorporating the previously generated tokens for context.\n",
    "\n",
    "In this notebook, we will implement an encoder-decoder model for sentiment analysis using the IMDB movie reviews dataset. The goal is to classify movie reviews as either positive or negative based on their content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b63d80-b70d-4df6-8f6b-237405adfa13",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Encoder-Decoder Architecture\n",
    "# ============================\n",
    "\n",
    "## Introduction to Encoder-Decoder Models\n",
    "\n",
    "Encoder-decoder models are a type of architecture used in various natural language processing (NLP) tasks, such as machine translation, text summarization, and more. These models consist of two main components: the encoder and the decoder.\n",
    "\n",
    "### What is an Encoder?\n",
    "\n",
    "The encoder processes the input sequence and converts it into a fixed-size context vector that captures the semantic information of the input. This vector summarizes the input data, allowing the decoder to generate the output sequence.\n",
    "\n",
    "### What is a Decoder?\n",
    "\n",
    "The decoder takes the context vector produced by the encoder and generates the output sequence one token at a time. At each step, it uses the context vector along with the previously generated tokens to predict the next token in the sequence.\n",
    "\n",
    "### How the Encoder-Decoder Works\n",
    "\n",
    "1. **Encoding**: The input sequence is passed to the encoder, which processes it and produces a context vector.\n",
    "2. **Decoding**: The decoder uses the context vector and generates the output sequence token by token.\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Machine Translation**: Translating text from one language to another.\n",
    "- **Text Summarization**: Creating a summary of a longer piece of text.\n",
    "- **Image Captioning**: Generating textual descriptions for images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b712887-fe73-4395-bc1a-7fd29386e9e4",
   "metadata": {},
   "source": [
    "## Example: Encoder-Decoder for Sentiment Analysis using IMDB Dataset\n",
    "\n",
    "Here, we will build an encoder-decoder model using TensorFlow and Keras for sentiment analysis on the IMDB movie reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a8d36-c1ee-4cd5-b36e-0c82170ffce1",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# Encoder-Decoder Model for Sentiment Analysis\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46aa87e-5b0a-408e-9599-7f4128657aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 04:06:33.099799: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-28 04:06:33.139456: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC-NOTICE: GPU memory for this assignment is capped at 1024MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 04:06:35.109368: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Importing Necessary Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf37ea-347c-4e43-a74b-721abba76270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n",
      "x_train shape: (25000, 100), y_train shape: (25000,)\n",
      "x_test shape: (25000, 100), y_test shape: (25000,)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, None, 128)            1280000   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 128)            1280000   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 256),                394240    ['embedding[0][0]']           \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 256),          394240    ['embedding_1[0][0]',         \n",
      "                              (None, 256),                           'lstm[0][1]',                \n",
      "                              (None, 256)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 1)              257       ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3348737 (12.77 MB)\n",
      "Trainable params: 3348737 (12.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 162s 509ms/step - loss: 0.4755 - accuracy: 0.7763 - val_loss: 0.4159 - val_accuracy: 0.8201\n",
      "Epoch 2/10\n",
      "245/313 [======================>.......] - ETA: 32s - loss: 0.2861 - accuracy: 0.8931"
     ]
    }
   ],
   "source": [
    "# Load the IMDB dataset\n",
    "max_words = 10000  # Limit vocabulary size to the top 10,000 words\n",
    "max_len = 100  # Set maximum length of input sequences\n",
    "\n",
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# Print shapes of the training and testing data\n",
    "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}, y_test shape: {y_test.shape}')\n",
    "\n",
    "# Define parameters for the model\n",
    "embedding_dim = 128  # Dimension of the embedding vector\n",
    "latent_dim = 256      # Dimensionality of the LSTM layer\n",
    "\n",
    "# ----------------------------\n",
    "# Encoder\n",
    "# ----------------------------\n",
    "# Input layer for the encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "# Embedding layer to convert integer sequences to dense vectors\n",
    "encoder_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim)(encoder_inputs)\n",
    "# LSTM layer for encoding\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
    "# Save the states to use them in the decoder\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# ----------------------------\n",
    "# Decoder\n",
    "# ----------------------------\n",
    "# Input layer for the decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "# Embedding layer for the decoder\n",
    "decoder_embedding = Embedding(input_dim=max_words, output_dim=embedding_dim)(decoder_inputs)\n",
    "# LSTM layer for decoding\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "# Passing the encoder states to the decoder\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# Dense layer for generating output predictions\n",
    "decoder_dense = Dense(1, activation='sigmoid')  # For binary classification\n",
    "# Final output from the decoder\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# ----------------------------\n",
    "# Model Definition\n",
    "# ----------------------------\n",
    "# Define the model with encoder and decoder inputs\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model with Adam optimizer and binary cross-entropy loss\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare Decoder Input Data\n",
    "# ----------------------------\n",
    "# Prepare decoder input data (shifted input)\n",
    "decoder_input_data = np.zeros_like(x_train)  # Initialize array with zeros\n",
    "decoder_input_data[:, 1:] = x_train[:, :-1]  # Shift inputs by one position\n",
    "\n",
    "# ----------------------------\n",
    "# Fit the Model\n",
    "# ----------------------------\n",
    "# Fit the model on the training data\n",
    "model.fit([x_train, decoder_input_data], \n",
    "          np.expand_dims(y_train, axis=-1), \n",
    "          batch_size=64, epochs=10, validation_split=0.2)\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate the Model\n",
    "# ----------------------------\n",
    "# Prepare decoder input data for testing\n",
    "test_decoder_input_data = np.zeros_like(x_test)\n",
    "test_decoder_input_data[:, 1:] = x_test[:, :-1]  # Shift inputs for testing\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate([x_test, test_decoder_input_data], \n",
    "                                 np.expand_dims(y_test, axis=-1))\n",
    "\n",
    "# Print test results\n",
    "print(f'Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
