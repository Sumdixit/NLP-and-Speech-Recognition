{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5ebd32-9df6-4c9b-af48-e60b8e07be7c",
   "metadata": {},
   "source": [
    "# Language Preprocessing with Tokenization and Sequencing\n",
    "\n",
    "## Overview:\n",
    "\n",
    "The goal is to guide language preprocessing by covering essential steps like tokenization, sequencing, padding, and vocabulary indexing. The process includes text, sentence, and word tokenization, followed by converting text into sequences, adding padding for uniform input lengths, and building a vocabulary index. Example code and explanations make it practical for NLP applications.\n",
    "\n",
    "# ============================\n",
    "# Table of Contents\n",
    "# ============================\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Text Tokenization](#Text-Tokenization)\n",
    "    - Sentence Tokenization\n",
    "    - Word Tokenization\n",
    "3. [Sequencing and Padding](#Sequencing-and-Padding)\n",
    "4. [Vocabulary and Word Index](#Vocabulary-and-Word-Index)\n",
    "\n",
    "# ============================\n",
    "# 1. Introduction\n",
    "# ============================\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this code, we will cover essential steps for text preprocessing in NLP: tokenization, sequencing, padding, and vocabulary indexing. These steps \n",
    "are foundational for preparing raw text data for machine learning and deep learning models, particularly when using neural networks for text \n",
    "classification, sentiment analysis, and other NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d130b4c6-3d97-463b-be7c-c802e82f412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for text preprocessing and tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK tokenizers if needed (run only once)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6d20fb-258b-4a6e-afe8-ba57516bfa96",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 2. Text Tokenization\n",
    "# ============================\n",
    "\n",
    "\n",
    "## Text Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, called tokens, which can be words, sentences, or even characters. For NLP tasks, \n",
    "sentence and word tokenization are commonly used to process text and understand its structure. We will use NLTK for both sentence and word \n",
    "tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c3fddf-cec8-452b-a269-aec89eaef5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample text for tokenization\n",
    "text = \"Natural Language Processing is fascinating. We will learn about tokenization today!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6df9e6-bced-4bb7-af73-68f270194e65",
   "metadata": {},
   "source": [
    "# ----------------------------------------\n",
    "# 2.1 Sentence Tokenization\n",
    "# ----------------------------------------\n",
    "\n",
    "### Sentence Tokenization\n",
    "\n",
    "Sentence tokenization is the process of splitting a block of text into individual sentences. This technique is a fundamental step in natural language processing (NLP) and is crucial for various applications that require sentence-level analysis or understanding.\n",
    "\n",
    "#### Importance of Sentence Tokenization\n",
    "\n",
    "- **Understanding Structure**: By dividing text into sentences, we can better understand the structure and flow of information. This is particularly important in documents where the meaning may change depending on the sentence context.\n",
    "- **Text Analysis**: Many NLP tasks, such as sentiment analysis, require understanding the sentiment expressed in individual sentences. Sentence tokenization allows models to process each sentence independently.\n",
    "- **Facilitating Summarization**: In text summarization, breaking down text into sentences enables algorithms to identify the most important sentences and condense information effectively.\n",
    "- **Enhancing Machine Learning Models**: Many machine learning models perform better when trained on sentence-level data rather than raw text. Sentence tokenization helps prepare data for such tasks.\n",
    "\n",
    "#### Use Cases\n",
    "\n",
    "- **Sentiment Analysis**: Understanding the sentiment expressed in customer reviews, where each sentence can convey a different sentiment.\n",
    "- **Information Retrieval**: Extracting specific sentences that answer user queries from larger documents.\n",
    "- **Chatbots**: Parsing user input to understand commands or questions at the sentence level.\n",
    "\n",
    "Below is an image that visually represents the sentence tokenization process, illustrating how a paragraph of text is broken down into individual sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6881a-1fe2-4b70-89b3-8c1a5ff37bf4",
   "metadata": {},
   "source": [
    "![Image Description](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/NLP_November/Lesson%205/sentence%20tokenization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b838aa-6eaa-42d4-b744-ae9a1c7c238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization: ['Natural Language Processing is fascinating.', 'We will learn about tokenization today!']\n"
     ]
    }
   ],
   "source": [
    "# Sentence tokenization - splitting text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd64e0-9880-4e43-8ac6-b15dae416fa7",
   "metadata": {},
   "source": [
    "# ----------------------------------------\n",
    "# 2.2 Word Tokenization\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "### Word Tokenization\n",
    "\n",
    "Word tokenization splits each sentence into words, allowing us to analyze text at the word level. This is especially useful for models \n",
    "that rely on word-level inputs, like bag-of-words or word embeddings.\n",
    "\n",
    "![Image Description](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/NLP_November/Lesson%205/word%20tokenization.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e75f42-8b2b-43a0-a900-153db9f6fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: [['Natural', 'Language', 'Processing', 'is', 'fascinating', '.'], ['We', 'will', 'learn', 'about', 'tokenization', 'today', '!']]\n"
     ]
    }
   ],
   "source": [
    "# Word tokenization - splitting each sentence into words\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "print(\"Word Tokenization:\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4577fa2e-df4a-40c0-9f12-d2d4d6e20499",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 3. Sequencing and Padding\n",
    "# ============================\n",
    "\n",
    "## Sequencing and Padding\n",
    "\n",
    "Once tokenization is complete, the next step is to convert the tokens (words) into numerical sequences. Machine learning models operate on numbers rather than raw text, making it essential to convert words into numerical representations. \n",
    "\n",
    "### What is Padding?\n",
    "\n",
    "Padding is the process of adding extra tokens to sequences to ensure they are of uniform length. In many machine learning models, especially in deep learning architectures like recurrent neural networks (RNNs) or convolutional neural networks (CNNs), input sequences must have the same length. Padding helps maintain this consistency.\n",
    "\n",
    "### Why Padding is Necessary\n",
    "\n",
    "- **Uniform Input Size**: Neural networks require inputs of the same shape. Padding ensures that all sequences are the same length, enabling batch processing.\n",
    "- **Efficient Training**: Uniform length sequences allow for efficient use of computational resources and optimize the training process.\n",
    "- **Preventing Information Loss**: When sequences are truncated to fit a uniform size, padding can help maintain critical information from longer sequences.\n",
    "\n",
    "### Types of Padding\n",
    "\n",
    "1. **Pre-padding**: Adding tokens to the beginning of sequences.\n",
    "2. **Post-padding**: Adding tokens to the end of sequences.\n",
    "\n",
    "### Example of Padding\n",
    "\n",
    "Consider the following sentences that have been tokenized into numerical sequences:\n",
    "\n",
    "- Sentence 1: \"I love NLP\" -> [1, 2, 3] \n",
    "- Sentence 2: \"Deep learning is fascinating\" -> [4, 5, 6, 7, 8]\n",
    "\n",
    "**Before Padding:**\n",
    "- Sequence lengths:\n",
    "  - Sentence 1: 3\n",
    "  - Sentence 2: 5\n",
    "\n",
    "To create uniform length sequences of 5, we can apply **post-padding**:\n",
    "\n",
    "**After Post-padding:**\n",
    "- Sentence 1: [1, 2, 3, 0, 0] (padded with two zeros)\n",
    "- Sentence 2: [4, 5, 6, 7, 8] (remains unchanged)\n",
    "\n",
    "### Example of Truncating\n",
    "\n",
    "In some cases, we might encounter sequences that are too long. To handle these, truncating may be necessary, which involves cutting off excess tokens to fit a specified length.\n",
    "\n",
    "For example, if we want to limit our sequences to a maximum length of 4:\n",
    "\n",
    "- Sentence 2: \"Deep learning is fascinating\" -> [4, 5, 6, 7, 8]\n",
    "\n",
    "**Before Truncating:**\n",
    "- Length: 5\n",
    "\n",
    "**After Truncating to 4:**\n",
    "- Sentence 2: [4, 5, 6, 7] (the last token \"8\" is removed)\n",
    "\n",
    "### Summary\n",
    "\n",
    "Padding and truncating are crucial preprocessing steps in NLP that ensure sequences have a uniform length, which is necessary for efficient processing in machine learning models. By carefully managing the lengths of sequences, we can preserve important information while also optimizing model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01453039-7577-4ea6-81be-143a0a6781a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus of text data\n",
    "corpus = [\n",
    "    \"I love machine learning.\",\n",
    "    \"NLP is fun and exciting.\",\n",
    "    \"Deep learning applications are vast.\"\n",
    "]\n",
    "\n",
    "# Initialize the Tokenizer and fit on text\n",
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")  # Using an out-of-vocabulary (OOV) token for words not seen in training\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088bb21-881d-436b-a0a2-efbdfc01d536",
   "metadata": {},
   "source": [
    "# ----------------------------------------\n",
    "# 3.1 Tokenization to Sequences\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "### Tokenization to Sequences\n",
    "\n",
    "Here, we use the `texts_to_sequences` method to convert the corpus into sequences of integers, each representing a unique word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc418e0-117f-46bf-b00c-e7d76c12b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sequences: [[3, 4, 5, 2], [6, 7, 8, 9, 10], [11, 2, 12, 13, 14]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing and converting text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(\"Tokenized Sequences:\", sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0da8e-2953-4275-92bb-fde54536691e",
   "metadata": {},
   "source": [
    "# ----------------------------------------\n",
    "# 3.2 Padding Sequences\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "### Padding Sequences\n",
    "\n",
    "Padding is added to ensure all sequences have the same length. This is necessary for feeding data into deep learning models that expect uniform \n",
    "input shapes. We use `post` padding to add padding at the end of sequences, though `pre` padding is also an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b516b0f-3503-47be-8ef7-7e2c6e03857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Sequences:\n",
      " [[ 3  4  5  2  0]\n",
      " [ 6  7  8  9 10]\n",
      " [11  2 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "# Define the max length for padding sequences (uniform length)\n",
    "max_length = 5\n",
    "\n",
    "# Pad sequences to ensure uniform input shape\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "print(\"Padded Sequences:\\n\", padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9977265-25ba-4947-a8a6-5c7e238229d2",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# 4. Vocabulary and Word Index\n",
    "# ============================\n",
    "\n",
    "\n",
    "## Vocabulary and Word Index\n",
    "\n",
    "After tokenization, the tokenizer creates a word index, mapping each unique word in the corpus to a unique integer ID. This index forms \n",
    "the vocabulary of our text data and is useful for converting new text into sequences that our model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5c3bb7-b5ec-49f5-80b1-5b589911f3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index:\n",
      " {'<OOV>': 1, 'learning': 2, 'i': 3, 'love': 4, 'machine': 5, 'nlp': 6, 'is': 7, 'fun': 8, 'and': 9, 'exciting': 10, 'deep': 11, 'applications': 12, 'are': 13, 'vast': 14}\n",
      "Vocabulary Size: 15\n"
     ]
    }
   ],
   "source": [
    "# View the word index created by the tokenizer\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Word Index:\\n\", word_index)\n",
    "\n",
    "# Total vocabulary size (including OOV token)\n",
    "vocab_size = len(word_index) + 1  # +1 to account for the OOV token\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd4b8c-d35c-42f7-bd5d-84740e947c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
