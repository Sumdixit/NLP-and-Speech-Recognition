{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4zp0nX_pc-N",
    "tags": []
   },
   "source": [
    "<center> <font size = 24 color = 'steelblue'> <b>Machine Translation<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Description](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/NLP_November/Lesson%205/Machine%20translation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "The goal is to build a machine translation pipeline by leveraging embeddings to translate an English dictionary to French. It involves loading necessary libraries and embeddings, working with embedding vectors, and using cosine similarity to measure semantic similarity. Additionally, gradient computation optimizes the transformation matrix for effective translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh2Ma8uxsz-2"
   },
   "source": [
    "# <a id= 'f0'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. Introduction](#f1)<br>\n",
    "[2. Loading libraries](#f2)<br>\n",
    "[3. Loading embeddings](#f3)<br>\n",
    "[4. Translating English dictionary to French](#f4)<br>\n",
    "> [4.1 Working with embeddings](#f4.1)<br>\n",
    "> [4.2 Computing the gradient of loss in respect to transform matrix R](#f4.2)<br>\n",
    "[3. Cosine Similarity](#f3)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRDRBXUIsz-3",
    "tags": []
   },
   "source": [
    "##### <a id = 'f1'>\n",
    "<font size = 10 color = 'midnightblue'> **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BunXF1nysz-4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Machine translation involves the use of automated systems to translate text or speech from one language to another.\n",
    "- NLP plays a crucial role in understanding, interpreting, and generating human language in a way that considers context and meaning.\n",
    "- NLP techniques are employed to enhance the quality and accuracy of machine translation systems.\n",
    "- NLP helps in addressing linguistic nuances, context understanding, and idiosyncrasies specific to each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZF0Mkagsz-6"
   },
   "source": [
    "##### <a id = 'f2'>\n",
    "<font size = 10 color = 'midnightblue'> **Load the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install below libraries if not done\n",
    "!pip install numpy==1.23.5\n",
    "!pip install tensorflow==2.13.1\n",
    "!pip install nltk==3.8.1\n",
    "!pip install pandas==1.5.3\n",
    "!pip install gensim==4.2.0\n",
    "!pip install scipy==1.9.3\n",
    "!pip install matplotlib==3.6.3\n",
    "!pip install scikit-learn==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RGOGrkD1yLk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pdb\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "import time\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj03n-wl1z3h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7c6UCC3zKD-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkHgR7CHzP20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhkSOPOn58OA",
    "tags": []
   },
   "source": [
    "##### <a id = 'f3'>\n",
    "<font size = 10 color = 'midnightblue'> **Load English and French Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6cBUUrM5cn_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_emb_subset = pickle.load(open(\"en_embeddings.p\", 'rb'))\n",
    "fr_emb_subset = pickle.load(open(\"fr_embeddings.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "uDYY_b_V8ptE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "file =  pd.read_csv('en-fr.train.txt', delimiter = ' ', header =None, index_col = [0]).squeeze('columns')\n",
    "eng_to_fr_dict_train =  file.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "J0WaJ7G17Zz0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(eng_to_fr_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz8kkd-2JmQg"
   },
   "outputs": [],
   "source": [
    "file2 =  pd.read_csv('en-fr.test.txt', delimiter = ' ', header =None, index_col = [0]).squeeze('columns')\n",
    "eng_to_fr_dict_test =  file2.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toxK9EqJJoer"
   },
   "outputs": [],
   "source": [
    "len(en_emb_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzs5FbZVsz_I"
   },
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-6G5c6s83CD",
    "tags": []
   },
   "source": [
    "##### <a id= 'f4'>\n",
    "<font size = 10 color = 'midnightblue'> **Translating English Dictionary to French** <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXagHx3tsz_K"
   },
   "source": [
    "##### <a id = 'f4.1'>\n",
    "<font size = 6 color = 'pwdrblue'> <b>Working with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kV1MOltsz_M"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Generate a matrix where where the columns are the English embeddings.\n",
    "- Generate a matrix where the columns correspond to the French embeddings.\n",
    "- Generate the projection matrix that minimizes the F norm ||X R -Y||^2.\n",
    "\n",
    "> - The goal is often to find a transformation matrix that minimizes the difference between two matrices.\n",
    "> - The Frobenius norm is a way to measure the \"size\" or magnitude of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tW6J_l9R7rtA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the set of words of English\n",
    "\n",
    "eng_words = en_emb_subset.keys()\n",
    "fr_words = fr_emb_subset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzRmvJM3_VHO"
   },
   "source": [
    "<font size = 5 color = 'seagreen'> <b>Check whether embedding is present for both the English and French words present in translations dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HZS3o98g-ueH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "eng_emb =[]\n",
    "frnch_emb = []\n",
    "\n",
    "for eng, fr in eng_to_fr_dict_train.items():\n",
    "    if (eng in eng_words) and (fr in fr_words):\n",
    "       # get the embeddings and store\n",
    "        eng_emb.append(en_emb_subset[eng])\n",
    "        frnch_emb.append(fr_emb_subset[fr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCprxnHGCxgW"
   },
   "source": [
    "<font size = 5 color = 'seagreen'> <b>Create English and French Embedded Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kzk-gEz2BY31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.vstack(eng_emb)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2jbaWHjMB7tw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y = np.vstack(frnch_emb)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb_IWtZSC8B7"
   },
   "source": [
    "<font size = 5 color = 'seagreen'> <b>Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWn3-w5Fsz_W"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "The loss function will be squared Frobenius norm of the difference between\n",
    "matrix and its approximation, divided by the number of training examples $m$.\n",
    "</div>\n",
    "\n",
    "<font size = 5>\n",
    "$$ L(X, Y, R)=\\frac{1}{m}\\sum_{i=1}^{m} \\sum_{j=1}^{n}\\left( a_{i j} \\right)^{2}$$\n",
    "\n",
    "\n",
    "<font size = 4>\n",
    "    \n",
    "<center> where $a_{i j}$ is value in $i$th row and $j$th column of the matrix $\\mathbf{XR}-\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kswNRk4mHrjs"
   },
   "source": [
    "##### <a id = 'f4.2'>\n",
    "<font size = 6 color = 'pwdrblue'> <b>Computing the gradient of loss in respect to transform matrix R\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "* Calculate the gradient of the loss with respect to transform matrix `R`.\n",
    "* The gradient is a matrix that encodes how much a small change in `R`\n",
    "affect the change in the loss function.\n",
    "* The gradient gives us the direction in which we should decrease `R`\n",
    "to minimize the loss.\n",
    "* $m$ is the number of training examples (number of rows in $X$).\n",
    "* The formula for the gradient of the loss function $𝐿(𝑋,𝑌,𝑅)$ is:\n",
    "\n",
    "$$\\frac{d}{dR}𝐿(𝑋,𝑌,𝑅)=\\frac{d}{dR}\\Big(\\frac{1}{m}\\| X R -Y\\|_{F}^{2}\\Big) = \\frac{2}{m}X^{T} (X R - Y)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0RGd3H3sz_n"
   },
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code implements a simple gradient descent algorithm to optimize a transformation matrix R for minimizing the mean squared error (MSE) between the transformed input data X @ R and the target data Y.\n",
    "\n",
    "#### Initialization:\n",
    "\n",
    "- A random matrix R of shape (X.shape[1], X.shape[1]).\n",
    "- Number of training steps (train_steps = 600) and learning rate (learning_rate = 0.8).\n",
    "\n",
    "#### Training Loop:\n",
    "\n",
    "- For every even iteration, the code computes the loss (MSE) and prints it.\n",
    "- The gradient of the loss with respect to R is computed and used to update R in the direction that reduces the loss.\n",
    "This process iteratively adjusts R to minimize the difference between the predicted and actual values in Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7mIx9HgK41c"
   },
   "outputs": [],
   "source": [
    "R =  np.random.rand(X.shape[1], X.shape[1])\n",
    "train_steps =600\n",
    "learning_rate  = 0.8\n",
    "\n",
    "for i in range(train_steps+1):\n",
    "    if i%2 ==0:\n",
    "        diff = (X @ R) -Y\n",
    "        sq_diff = diff**2\n",
    "        loss = np.sum(sq_diff)/X.shape[0]\n",
    "        print(f\"loss at iteration {i} is : {loss:.3f}\")\n",
    "    gradient =  np.dot(X.transpose(),np.dot(X,R) -Y)*(2/X.shape[0])\n",
    "    R = R - learning_rate*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L08pypBAK-DT"
   },
   "outputs": [],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MiHNam6sz_p"
   },
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBVtVVCiYTJH"
   },
   "outputs": [],
   "source": [
    "pred = np.dot(X,R) # finding dot product between X and R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q488_RYnL-WD"
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code checks how well a transformation matrix R maps input data X to target data Y using cosine similarity.\n",
    "\n",
    "- cosine_similarity(): Calculates how similar two vectors are.\n",
    "\n",
    "- nearest_neighbor(): Finds the vector in a list (candidates) that is most similar to a given vector v.\n",
    "\n",
    "- test_vocab():\n",
    "\n",
    ">> - Transforms X using the matrix R.\n",
    ">> - For each transformed vector, it finds the closest match in Y.\n",
    ">> - Measures the accuracy by checking if the closest match is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sct4VBN1MI3X"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "\n",
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    return np.argsort([cosine_similarity(v,row) for row in candidates])[-k:]\n",
    "\n",
    "def test_vocab(X,Y,R):\n",
    "    pred = np.dot(X,R)\n",
    "    return sum([nearest_neighbor(row,Y)==index for index, row in enumerate(pred)])/len(pred)\n",
    "\n",
    "test_vocab(X,Y,R)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
