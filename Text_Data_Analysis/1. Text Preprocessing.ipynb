{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b726cfeb-e2da-4d2f-b04f-7f48b0f411d4",
   "metadata": {
    "id": "b726cfeb-e2da-4d2f-b04f-7f48b0f411d4",
    "tags": []
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> <b>Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e658a0-5ca4-4934-b354-7ecf597435b7",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "The goal is to understand and implement the essential steps of text preprocessing using Python. The notebook covers data cleaning tasks like tokenization, case normalization, spelling correction, POS tagging, and NER. It also includes stemming, lemmatization, and noise removal, such as stopwords, URLs, punctuations, and emoticons, to prepare text data for NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33217b9d-dd39-4643-bb3b-188c4fb4188c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "- Understand steps involved in text preprocessing\n",
    "- Implement text oreprocessing using  python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437063e-f8bd-41f1-89e9-53da3e9d3452",
   "metadata": {},
   "source": [
    "# <a id= 't0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of contents:**<br>\n",
    "[1. Installation and import of necessary packages](#t1)<br>\n",
    "[2. Download the necessary corpus from NLTK](#t2)<br>\n",
    "[3. Data cleaning steps](#t3)<br>\n",
    "> [3.1 Tokenization](#t3.1)<br>\n",
    "> [3.2 Changing case](#t3.2)<br>\n",
    "> [3.3 Spelling correction](#t3.3)<br>\n",
    "> [3.4 POS Tagging](#t3.4)<br>\n",
    "> [3.5 Named entity recognition (NER)](#t3.5)<br>\n",
    "> [3.6 Stemming and Lemmatization](#t3.6)<br>\n",
    ">> [a. Stemming](#3a)<br>\n",
    ">> [b. Lemmatization](#3b)<br>\n",
    "\n",
    "> [3.7 Noise entity removal](#t3.7)<br>\n",
    ">> [a. Remove stopwords](#a)<br>\n",
    ">> [b. Remove urls](#b)<br>\n",
    ">> [c. Remove punctuations](#c)<br>\n",
    ">> [d. Remove emoticons](#d)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327445b-ee2e-4e1f-8cd0-4585b7988a9c",
   "metadata": {
    "id": "b327445b-ee2e-4e1f-8cd0-4585b7988a9c"
   },
   "source": [
    "##### <a id = 't1'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Installation and import of necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55bdf780-7cdd-40b4-bd08-4512dd524453",
   "metadata": {
    "id": "55bdf780-7cdd-40b4-bd08-4512dd524453",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1) (2022.6.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk==3.8.1) (4.64.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy==3.5.1 in /usr/local/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (6.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /voc/work/.local/lib/python3.10/site-packages (from spacy==3.5.1) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (22.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from spacy==3.5.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy==3.5.1) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.5.1) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy==3.5.1) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->spacy==3.5.1) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for string\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/usr/local/venvs/jupyter/bin/python: No module named spacy\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting svgling\n",
      "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting svgwrite (from svgling)\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
      "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
      "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m963.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
      "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py:617: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/usr/local/lib/python3.10/site-packages/thinc/compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "/usr/local/lib/python3.10/site-packages/thinc/compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n",
      "2024-10-30 07:29:47.384554: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 07:29:47.397755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-30 07:29:47.412627: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-30 07:29:47.416987: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 07:29:47.428752: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 07:29:54.021891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-10-30 07:30:02.321602: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/site-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.4.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /voc/work/.local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (58.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (22.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.8.1\n",
    "!pip install spacy==3.5.1\n",
    "!pip install re\n",
    "!pip install string\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install svgling\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f344be-7dcf-4b70-bac9-343b924b0924",
   "metadata": {
    "id": "58f344be-7dcf-4b70-bac9-343b924b0924",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852dc234-cf9f-408c-a56c-da56408a6f3c",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ef5bd-796d-4cbd-acd2-1be72b2d08c6",
   "metadata": {
    "id": "de0ef5bd-796d-4cbd-acd2-1be72b2d08c6"
   },
   "source": [
    "##### <a id = 't2'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Download necessary corpus and models from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df6279c-fd1f-420e-955b-62563f75ab50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1701922141715,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "0df6279c-fd1f-420e-955b-62563f75ab50",
    "outputId": "61da519c-23bd-4855-91f8-5be3e610fcbc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /voc/work/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /voc/work/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /voc/work/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package words to /voc/work/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /voc/work/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package stopwords to /voc/work/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /voc/work/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pelwBrw-oZpf",
   "metadata": {
    "id": "pelwBrw-oZpf"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**Note:**\n",
    "    \n",
    "- A LoadError will be raised whenever there is a missing corpus or model which is a dependency for some other function.\n",
    "- Use `nltk.download( <name of the corpus/model> )` for downloading the requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b145e-0f14-4f57-9b29-960f2b6704ab",
   "metadata": {
    "id": "cd6b145e-0f14-4f57-9b29-960f2b6704ab"
   },
   "source": [
    "<font size = 6 color = seagreen> <b> Import the necessary corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6724b372-f09c-429e-89e2-0c9a0c3e1a4f",
   "metadata": {
    "id": "6724b372-f09c-429e-89e2-0c9a0c3e1a4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22600d1-b227-481f-8b8f-447c1cc134a8",
   "metadata": {},
   "source": [
    "##### <a id = 't3'>\n",
    "<font size = 10 color = 'midnightblue'> <b> Data cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6bc37-fb57-461c-b917-f0ec85084fdd",
   "metadata": {
    "id": "86d6bc37-fb57-461c-b917-f0ec85084fdd"
   },
   "source": [
    "<font size = 6 color = seagreen> <b> <center> Let's start by defining a custom text for preprocessing.<br>\n",
    "<font size = 6 color = seagreen> <center>This text contains emoticons, punctuations urls etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e248600-0cee-435e-b553-01df6ec46ec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1701922142190,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "2e248600-0cee-435e-b553-01df6ec46ec7",
    "outputId": "bdbed5f4-0c36-4001-ea57-887d870f7be2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embracing life's challenges is like navigating a journey. üöÄ\n",
      "Stay motivated, overcome hurdles, and explore new paths to success!\n",
      "Check out inspiring stories at https://motivationalhub.com for an extra boost!\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Embracing life's challenges is like navigating a journey. üöÄ\n",
    "Stay motivated, overcome hurdles, and explore new paths to success!\n",
    "Check out inspiring stories at https://motivationalhub.com for an extra boost!\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247bd17-d860-4fcb-b973-866b8f9c6664",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50756761-9f28-4c5d-ad9e-5cfa564dd6b7",
   "metadata": {
    "id": "b7e0ed52-ed1e-47d6-aeac-6c433ec43951",
    "tags": []
   },
   "source": [
    "<a id = 't3.1'>\n",
    "<font size = 6 color = pwdrblue>  <b>Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927e157-6de2-4d43-9c10-f22a2848291d",
   "metadata": {
    "id": "b7e0ed52-ed1e-47d6-aeac-6c433ec43951",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"font-size: 16px;\"> <!-- Set font size using CSS -->\n",
    "    <ul>\n",
    "        <li>Tokenization is the process of breaking down text into smaller components, known as tokens.</li>\n",
    "        <li>Tokens can be:\n",
    "            <ul>\n",
    "                <li>Words</li>\n",
    "                <li>Phrases</li>\n",
    "                <li>Symbols</li>\n",
    "                <li>Other meaningful elements</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>Tokenization is a foundational step in natural language processing (NLP).</li>\n",
    "        <li>It transforms unstructured text into a structured format that algorithms can analyze and manipulate.</li>\n",
    "        <li> Sentence tokenization further divides text into individual sentences for better context understanding.</li> <!-- Added line for sentence tokenization -->\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f254f-8497-4240-b09a-96032e58a5af",
   "metadata": {},
   "source": [
    "![Image Description](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/NLP/tokenization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea73ff-3a0f-448e-8bf4-63aa04d942af",
   "metadata": {
    "id": "c7ea73ff-3a0f-448e-8bf4-63aa04d942af"
   },
   "source": [
    "<font size = 5 color = seagreen>  <b>Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60d4649-0948-4b04-93b8-15e3f441bf2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922142190,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "e60d4649-0948-4b04-93b8-15e3f441bf2b",
    "outputId": "1c9b4bd1-fed9-4c55-fe05-630c2e5955bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embracing', 'life', \"'s\", 'challenges', 'is', 'like', 'navigating', 'a', 'journey', '.', 'üöÄ', 'Stay', 'motivated', ',', 'overcome', 'hurdles', ',', 'and', 'explore', 'new', 'paths', 'to', 'success', '!', 'Check', 'out', 'inspiring', 'stories', 'at', 'https', ':', '//motivationalhub.com', 'for', 'an', 'extra', 'boost', '!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a3e24-ef79-414d-adc4-1ada1c211b5e",
   "metadata": {
    "id": "a50a3e24-ef79-414d-adc4-1ada1c211b5e"
   },
   "source": [
    "<font size = 5 color = seagreen>  <b>Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72cbc39-a51b-42f0-ac0f-35a959ec40ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701922142190,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "b72cbc39-a51b-42f0-ac0f-35a959ec40ca",
    "outputId": "95a1cacd-b02d-4348-9117-31dc86781fb2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  Embracing life's challenges is like navigating a journey.\n",
      "1:  üöÄ\n",
      "Stay motivated, overcome hurdles, and explore new paths to success!\n",
      "2:  Check out inspiring stories at https://motivationalhub.com for an extra boost!\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"{i}:  {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa24a6-b9ca-45af-8b1f-925bb60b2fd6",
   "metadata": {
    "id": "efaa24a6-b9ca-45af-8b1f-925bb60b2fd6"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "**However, if the text contains emoticons or URLs, word tokenization may split them, complicating the text cleaning process. Hence, a simple text split function could be more helpful in this context.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8baf231-8187-473a-90ee-b8e018efa15a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922142191,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "d8baf231-8187-473a-90ee-b8e018efa15a",
    "outputId": "159d23b0-530c-4e19-a6f2-ffea21f8013a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embracing', \"life's\", 'challenges', 'is', 'like', 'navigating', 'a', 'journey.', 'üöÄ', 'Stay', 'motivated,', 'overcome', 'hurdles,', 'and', 'explore', 'new', 'paths', 'to', 'success!', 'Check', 'out', 'inspiring', 'stories', 'at', 'https://motivationalhub.com', 'for', 'an', 'extra', 'boost!']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = text.split()\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadabc46-045a-4e12-9cb5-62a582ab4566",
   "metadata": {
    "id": "cadabc46-045a-4e12-9cb5-62a582ab4566"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- <b>This also creates word tokens but keeps emoticons, urls, address handles, and hastags etc. together for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042e4a0-7dad-491f-8024-0fd27b0c7873",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60446923-8cab-43ec-8f30-ccce30596799",
   "metadata": {
    "id": "d0c48cb4-f273-4b2e-8a6c-3cf39774e289"
   },
   "source": [
    "<a id = 't3.2'>\n",
    "<font size = 6 color = pwdrblue>  <b>Changing the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c52234-01ac-4fb8-9f83-e7dc80baa732",
   "metadata": {
    "id": "d0c48cb4-f273-4b2e-8a6c-3cf39774e289"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Change of case is a text normalization process.\n",
    "- This process provides for uniform representation and reduces the vocabulary size.\n",
    "- Casing also eases the process of text matching, entity recognition, search and retrieval.\n",
    "- Changing the casing of the data reduces redundancy and helps the ML model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f77a48b-aa48-4576-a615-9271439ae6bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922142191,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "6f77a48b-aa48-4576-a615-9271439ae6bb",
    "outputId": "f392307d-c354-467c-d957-84a416d661b0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embracing', \"life's\", 'challenges', 'is', 'like', 'navigating', 'a', 'journey.', 'üöÄ', 'stay', 'motivated,', 'overcome', 'hurdles,', 'and', 'explore', 'new', 'paths', 'to', 'success!', 'check', 'out', 'inspiring', 'stories', 'at', 'https://motivationalhub.com', 'for', 'an', 'extra', 'boost!']\n"
     ]
    }
   ],
   "source": [
    "words_lower_case = text.lower().split()\n",
    "print(words_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3870783-ebfd-43cc-a8b4-8ef5be5af7e1",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1a004-e48d-46f1-ba54-063259c02c7c",
   "metadata": {
    "id": "adb51ddc-5eaa-4ec6-9e72-0365705a51fd"
   },
   "source": [
    "<a id = 't3.3'>\n",
    "<font size = 6 color = pwdrblue>  <b>Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a935e4b-418c-4b9b-b4e1-9ac0c106a096",
   "metadata": {
    "id": "27861661-efaa-49a4-98e0-a66bb29da4e8",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- This improves text quality and avoids miscommunication.\n",
    "- Spell correction helps support language models and embeddings.\n",
    "- Spell correction helps reducing ambiguity and handle out of vocabulary data.\n",
    "\n",
    "**For spelling correction we are using:**\n",
    " - `nltk.edit_distance` to measure distance between the words in the text and the vocabulary available in nltk.\n",
    " - `edit_distance` calculate the `Levenshtein edit-distance` between two strings to check similarity between words in the text and words of the valid vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ff6a66-d569-47db-9980-c1e257e6ac2a",
   "metadata": {
    "id": "a3ff6a66-d569-47db-9980-c1e257e6ac2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "word_tokens = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220e8f93-5f1f-4cbc-b40b-22aa6e8faffb",
   "metadata": {
    "id": "220e8f93-5f1f-4cbc-b40b-22aa6e8faffb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get list of English words\n",
    "words = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb872def-906c-4cf5-878d-c7bd806607df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701922142191,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "fb872def-906c-4cf5-878d-c7bd806607df",
    "outputId": "7c648468-96d7-4c26-c991-4d1ffac8bf55",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the vocabulary :  236736\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words in the vocabulary : \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e33d82e-a944-4cc4-bb31-2174d7522d42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 586186,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "9e33d82e-a944-4cc4-bb31-2174d7522d42",
    "outputId": "54553cd5-0aee-4513-d4cb-4d4fceb9bfcf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected tokens: ['embracing', 'life', 'challenge', 'is', 'like', 'navigation', 'a', 'journey', 'A', 'stay', 'motivate', 'overcome', 'hurdies', 'and', 'explore', 'new', 'patas', 'to', 'success', 'check', 'out', 'inspiring', 'storied', 'at', 'motivational', 'for', 'an', 'extra', 'boost']\n"
     ]
    }
   ],
   "source": [
    "# Correct spelling of each word\n",
    "corrected_tokens = []\n",
    "for token in word_tokens:\n",
    "    # Find the word with the lowest distance and replace it\n",
    "    corrected_token = min(words, key=lambda x: nltk.edit_distance(x, token))\n",
    "    corrected_tokens.append(corrected_token)\n",
    "print(\"Corrected tokens:\", corrected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deed5ba-a6e2-4f2a-bff3-53c59db5238d",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc860cb-f3ca-4b46-8b4d-3cd3d4a19954",
   "metadata": {
    "id": "392de69a-8c76-4312-88d2-b222660319f2"
   },
   "source": [
    "<a id = 't3.4'>\n",
    "<font size = 6 color = pwdrblue>  <b>POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa8156-e0ae-4900-9bb3-58680e81954b",
   "metadata": {
    "id": "392de69a-8c76-4312-88d2-b222660319f2"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "Part-of-Speech tagging involves assigning words in a text corpus to specific parts of speech based on their definitions and contextual usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d9c2694-9413-4225-b05d-abbd66625ada",
   "metadata": {
    "id": "3d9c2694-9413-4225-b05d-abbd66625ada",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "word_tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dba6a528-4bf7-4c96-8ed7-35a6c69623a5",
   "metadata": {
    "id": "dba6a528-4bf7-4c96-8ed7-35a6c69623a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part-of-speech tagging can be done using pos_tag function of nltk.\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f56e106a-37ef-4815-87fd-f57ef786d285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "f56e106a-37ef-4815-87fd-f57ef786d285",
    "outputId": "d74c71c9-6c21-4de6-bd36-efc47a86df58",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Embracing', 'VBG'), (\"life's\", 'NN'), ('challenges', 'NNS'), ('is', 'VBZ'), ('like', 'IN'), ('navigating', 'VBG'), ('a', 'DT'), ('journey.', 'NN'), ('üöÄ', 'NNP'), ('Stay', 'NNP'), ('motivated,', 'VBZ'), ('overcome', 'JJ'), ('hurdles,', 'NN'), ('and', 'CC'), ('explore', 'VB'), ('new', 'JJ'), ('paths', 'NNS'), ('to', 'TO'), ('success!', 'VB'), ('Check', 'NNP'), ('out', 'RP'), ('inspiring', 'VBG'), ('stories', 'NNS'), ('at', 'IN'), ('https://motivationalhub.com', 'NN'), ('for', 'IN'), ('an', 'DT'), ('extra', 'JJ'), ('boost!', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ae1b3-044d-4312-a520-fe5ce81deb00",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334f797-0000-40fa-a6a9-165ec3bf8631",
   "metadata": {
    "id": "cd8d504b-a253-4323-afe8-abe7c811e933"
   },
   "source": [
    "<a id = 't3.5'>\n",
    "<font size = 6 color = pwdrblue>  <b>Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d0447-356f-483f-ab03-909cf41add00",
   "metadata": {
    "id": "cd8d504b-a253-4323-afe8-abe7c811e933"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Named entity recognition (NER) is a natural language processing (NLP) technique that involves identifying and classifying entities (objects, places, people, organizations, dates, monetary values, percentages, etc.) in text.\n",
    "- Named entities can belong to various categories, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fae75-989a-4fbb-8a7f-758716b74ab9",
   "metadata": {
    "id": "cd8d504b-a253-4323-afe8-abe7c811e933"
   },
   "source": [
    "|**Entity Object**| **Meaning** |\n",
    "|-|-|\n",
    "|Person |Individual names of people.|\n",
    "|Location| Places, cities, countries, etc.|\n",
    "|Organization | Names of companies, institutions, etc.|\n",
    "|Date | Temporal expressions like dates and times.|\n",
    "|Money| Currency amounts.|\n",
    "|Percent| Percentage values.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O0RgoHSXrIxv",
   "metadata": {
    "id": "O0RgoHSXrIxv"
   },
   "source": [
    "<font size = 5 color = seagreen> <b><center> Let's consider a different example text to understand named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25a3784c-85c2-4ed6-8c1e-6f9fb42b9dc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "25a3784c-85c2-4ed6-8c1e-6f9fb42b9dc2",
    "outputId": "9e692cb1-11a6-463d-fdba-349c34e4bc2a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 2019, Apple Inc. announced the launch of the iPhone 11 at their headquarters in Cupertino, California, with Tim Cook, the CEO, presenting the new features.\n"
     ]
    }
   ],
   "source": [
    "text_example = \"In 2019, Apple Inc. announced the launch of the iPhone 11 at their headquarters in Cupertino, California, with Tim Cook, the CEO, presenting the new features.\"\n",
    "print(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b12f928-89bd-42a8-8f1a-56aa0423a6cc",
   "metadata": {
    "id": "9b12f928-89bd-42a8-8f1a-56aa0423a6cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "word_tokens = text_example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5a436c8-492e-417a-b280-385096ce9a6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "e5a436c8-492e-417a-b280-385096ce9a6d",
    "outputId": "c1810e58-c329-4e3e-8f23-a3fa605d4644",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('2019,', 'CD'), ('Apple', 'NNP'), ('Inc.', 'NNP'), ('announced', 'VBD'), ('the', 'DT'), ('launch', 'NN'), ('of', 'IN'), ('the', 'DT'), ('iPhone', 'NN'), ('11', 'CD'), ('at', 'IN'), ('their', 'PRP$'), ('headquarters', 'NNS'), ('in', 'IN'), ('Cupertino,', 'NNP'), ('California,', 'NNP'), ('with', 'IN'), ('Tim', 'NNP'), ('Cook,', 'NNP'), ('the', 'DT'), ('CEO,', 'NNP'), ('presenting', 'VBG'), ('the', 'DT'), ('new', 'JJ'), ('features.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# get the pos tags\n",
    "tagged = nltk.pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "493befe4-99e9-4cbe-a00a-f5ca391f76ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "493befe4-99e9-4cbe-a00a-f5ca391f76ae",
    "outputId": "93084bb4-b1cd-4194-e100-9cb52a7f13aa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  2019,/CD\n",
      "  Apple/NNP\n",
      "  Inc./NNP\n",
      "  announced/VBD\n",
      "  the/DT\n",
      "  launch/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION iPhone/NN)\n",
      "  11/CD\n",
      "  at/IN\n",
      "  their/PRP$\n",
      "  headquarters/NNS\n",
      "  in/IN\n",
      "  Cupertino,/NNP\n",
      "  California,/NNP\n",
      "  with/IN\n",
      "  (PERSON Tim/NNP)\n",
      "  Cook,/NNP\n",
      "  the/DT\n",
      "  CEO,/NNP\n",
      "  presenting/VBG\n",
      "  the/DT\n",
      "  new/JJ\n",
      "  features./NN)\n"
     ]
    }
   ],
   "source": [
    "named_entities = nltk.ne_chunk(tagged)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HEA3gWWCr75A",
   "metadata": {
    "id": "HEA3gWWCr75A"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Named entity recognition can also be implemented using spcay packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daaca89d-1aa1-4b6c-9470-345ccd587595",
   "metadata": {
    "id": "daaca89d-1aa1-4b6c-9470-345ccd587595",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f540660-8342-4fa4-ab16-7c239a5900d1",
   "metadata": {
    "id": "5f540660-8342-4fa4-ab16-7c239a5900d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a nlp object of the text\n",
    "doc = nlp(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e92eef-b6b6-4b78-a130-fddccef678bd",
   "metadata": {
    "id": "70e92eef-b6b6-4b78-a130-fddccef678bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract named entities\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fef92581-9e2f-42a5-a3fd-857019310cbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922729092,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "fef92581-9e2f-42a5-a3fd-857019310cbb",
    "outputId": "9307f5c2-ea62-401c-b329-344222a8c65d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2019', 'DATE'), ('Apple Inc.', 'ORG'), ('11', 'CARDINAL'), ('Cupertino', 'GPE'), ('California', 'GPE'), ('Tim Cook', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Print the named entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e02fc1-ab0a-4dca-9026-029fdb6b616e",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349d3f6-a243-4b2c-b1d0-6e67b9274b76",
   "metadata": {
    "id": "96c84d31-f526-484c-9f96-68b491423eaa"
   },
   "source": [
    "<a id = 't3.6'>\n",
    "<font size = 6 color = pwdrblue>  <b>Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3cf7f-8e70-4893-87be-8ce0d2202fc5",
   "metadata": {
    "id": "96c84d31-f526-484c-9f96-68b491423eaa"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Stemming and lemmatization are techniques used in NLP and text mining to reduce words to their base or root forms, simplifying the process of analysis and text understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b398fd-a9bc-4006-85b8-1f25fd979639",
   "metadata": {
    "id": "25b398fd-a9bc-4006-85b8-1f25fd979639",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Embracing life's challenges is like navigating a journey. üöÄ\n",
    "Stay motivated, overcome hurdles, and explore new paths to success!\n",
    "Check out inspiring stories at https://motivationalhub.com for an extra boost!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6hu8QVeBszFb",
   "metadata": {
    "id": "6hu8QVeBszFb"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Let's start by tokenising the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1100190a-2996-4285-9691-7b901c14f4b1",
   "metadata": {
    "id": "1100190a-2996-4285-9691-7b901c14f4b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_tokens = text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa1258-a5ca-4378-abab-c10377c0526c",
   "metadata": {
    "id": "510323db-23a3-46b9-acc5-167fa8578f6c"
   },
   "source": [
    "<a id = '3a'>\n",
    "<font size = 5 color = seagreen> <b> Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec277cea-1a19-47ee-84f0-bbb186c6fc6c",
   "metadata": {
    "id": "510323db-23a3-46b9-acc5-167fa8578f6c"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Stemming is the process of removing suffixes or prefixes from words to obtain their root or base form, known as the stem. The goal is to reduce words to a common form, even if it is not a valid word.\n",
    "- Porter stemmer is one of the most used stemming technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b20c41f6-2360-411b-b5ee-6879b9f7c08e",
   "metadata": {
    "id": "b20c41f6-2360-411b-b5ee-6879b9f7c08e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create stemmer object\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11887788-9fbe-4ff7-956c-d078d95c7963",
   "metadata": {
    "id": "11887788-9fbe-4ff7-956c-d078d95c7963",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stem each token\n",
    "stemmed_tokens = [stemmer.stem(token) for token in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "579770bc-0dec-4b50-b7d8-3198d108760f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922729092,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "579770bc-0dec-4b50-b7d8-3198d108760f",
    "outputId": "58f9af2b-6ce4-4868-a003-d4ae02127d2b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: ['embrac', \"life'\", 'challeng', 'is', 'like', 'navig', 'a', 'journey.', 'üöÄ', 'stay', 'motivated,', 'overcom', 'hurdles,', 'and', 'explor', 'new', 'path', 'to', 'success!', 'check', 'out', 'inspir', 'stori', 'at', 'https://motivationalhub.com', 'for', 'an', 'extra', 'boost!']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350e9af-5f90-4aa2-8649-d61639718619",
   "metadata": {
    "id": "7c46765e-4f67-4f27-a0c3-e3188f982860"
   },
   "source": [
    "<a id = '3b'>\n",
    "<font size = 5 color = seagreen> <b> Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8b1ce-4017-4f09-91ec-2c5ea3250573",
   "metadata": {
    "id": "7c46765e-4f67-4f27-a0c3-e3188f982860"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma.\n",
    "- Lemmatization considers the context and meaning of a word and produces valid words.\n",
    "- NLTK provides wordnet based lemmatizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f01a3-d5be-4ab4-afec-041c166658d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"font-size: 16px; margin-top: 20px;\">\n",
    "    <h4>What is WordNet?</h4>\n",
    "    <p>WordNet is a large lexical database of English, where words are grouped into sets of synonyms called synsets. Each synset contains a word or phrase and provides definitions and examples of usage. It is widely used in natural language processing (NLP) for tasks such as semantic analysis and word sense disambiguation.</p>\n",
    "    <p>NLTK provides a WordNet-based lemmatizer that utilizes this resource to reduce words to their base or dictionary form (lemma), which is essential for tasks like text normalization.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "478ff88c-84b2-4ba4-ad16-54fe56308e5d",
   "metadata": {
    "id": "478ff88c-84b2-4ba4-ad16-54fe56308e5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lemmatizer object\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab3221-a966-45b7-963b-d7d14a6a1d79",
   "metadata": {
    "id": "83ab3221-a966-45b7-963b-d7d14a6a1d79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatize each token\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ca39e-6640-450d-85b7-94626bc87002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922731008,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "556ca39e-6640-450d-85b7-94626bc87002",
    "outputId": "32ef84bf-9534-4b9b-f3de-006f8a558a09",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4b602-9371-4e01-9b49-4216ae25cd13",
   "metadata": {
    "id": "66a4b602-9371-4e01-9b49-4216ae25cd13",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "**Using PoS tagging in lemmatization**\n",
    "  - For implementation of PoS tag based lemmatization, we pass the PoS tag for each word in the sentence.\n",
    "  - To acheive this we need to first map PoS tags from Penn Treebank to WordNet PoS tags.\n",
    "  - The below function performs the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ce1f9-0613-4acf-b89a-e419b2aa9320",
   "metadata": {
    "id": "b28ce1f9-0613-4acf-b89a-e419b2aa9320",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pos tag mapping\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd785f-fda2-4b9e-a48d-4868be7ddd67",
   "metadata": {
    "id": "2afd785f-fda2-4b9e-a48d-4868be7ddd67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the pos tag\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42423b44-f903-4fbf-8b82-05f3f1a5ee42",
   "metadata": {
    "id": "42423b44-f903-4fbf-8b82-05f3f1a5ee42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the root word for each of the tokens using their corresponding pos-tags\n",
    "lemma_sent = []\n",
    "for word, tag in tagged:\n",
    "    new_tag = pos_tagger(tag)\n",
    "    lemma = lemmatizer.lemmatize(word, new_tag)\n",
    "    lemma_sent.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fd56c-0a64-4fe8-8db6-825738f2f791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922731008,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "431fd56c-0a64-4fe8-8db6-825738f2f791",
    "outputId": "1d92ac6c-91cd-44e0-daef-162b2b0decbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Original sentence : \\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e137d-7464-4b5a-9166-afdaa50c2919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701922731008,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "a63e137d-7464-4b5a-9166-afdaa50c2919",
    "outputId": "631830ef-c799-4a7c-96d0-8774d3c500ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Lemmatized sentence : \\n{' '.join(lemma_sent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec58bb2-43dc-4a9d-a502-1581bba7589d",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada9427-1388-48a6-a2a3-893739fd3e86",
   "metadata": {
    "id": "497cd5de-ba5b-45df-b79c-03479894db5c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id = 't3.7'>\n",
    "<font size = 6 color = pwdrblue>  <b>Noise entity removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d922159-43bf-4aea-a8eb-f559bf0a781d",
   "metadata": {
    "id": "497cd5de-ba5b-45df-b79c-03479894db5c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Noise entity removal in NLP involves the identification and removal of irrelevant or undesired entities from a given text.\n",
    "- Noise entities can be entities that are not relevant to the analysis or entities that add unnecessary complexity to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1LqKYIBOvLpL",
   "metadata": {
    "id": "1LqKYIBOvLpL"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Embracing life's challenges is like navigating a journey. üöÄ\n",
    "Stay motivated, overcome hurdles, and explore new paths to success!\n",
    "Check out inspiring stories at https://motivationalhub.com for an extra boost!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc5a2d-31e8-49c0-adaa-bf4de5a2a3de",
   "metadata": {
    "id": "91dc5a2d-31e8-49c0-adaa-bf4de5a2a3de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "word_tokens = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ef019-97e2-4f5e-9f31-eb279cbdf38c",
   "metadata": {
    "id": "ce0ef019-97e2-4f5e-9f31-eb279cbdf38c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PoS tagging\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c94f62-3884-4177-8d3d-d8c0b6dad18d",
   "metadata": {
    "id": "c6c94f62-3884-4177-8d3d-d8c0b6dad18d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemma_sent = []\n",
    "for word, tag in tagged:\n",
    "    new_tag = pos_tagger(tag)\n",
    "    lemma = lemmatizer.lemmatize(word, new_tag)\n",
    "    lemma_sent.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace388e1-d33d-4ac1-9dd6-8c5b63b09232",
   "metadata": {},
   "source": [
    "<a id = 'a'>\n",
    "<font size = 5 color = seagreen> <b> a. Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600c2dc-13d4-45ab-82c3-ea7451e58278",
   "metadata": {
    "id": "c600c2dc-13d4-45ab-82c3-ea7451e58278"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- Identify and remove common stopwords (e.g., \"is,\" \"the,\" \"and\") that do not carry much semantic meaning.\n",
    "- This can help in focusing on more meaningful entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387f0be-56b2-4ad9-aa23-97c693be5798",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "5387f0be-56b2-4ad9-aa23-97c693be5798",
    "outputId": "a9888af7-2076-40e3-a5d5-948a1df1e3dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the list of stopwords from the corpus\n",
    "stp_wrds_eng = stopwords.words('english')\n",
    "print(stp_wrds_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d24e74-8391-47d0-8460-5a615dd9b3b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "49d24e74-8391-47d0-8460-5a615dd9b3b1",
    "outputId": "ea304338-ea6c-44d1-aa8f-4f7aabd2bc3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "text_clean = [w for w in lemma_sent if w not in stp_wrds_eng]\n",
    "print(f\"Lemmatized : \\n{' '.join(lemma_sent)}\")\n",
    "print(f\"Cleaned  : \\n{' '.join(text_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c20b65-bbe0-4bc3-8fe6-4d71d7c4ec83",
   "metadata": {
    "id": "c7826e48-8e1f-43d4-9899-4dc59cab7dbf"
   },
   "source": [
    "<a id = 'b'>\n",
    "<font size = 5 color = seagreen> <b> b. Removing urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3e15e-7287-4ef3-bc53-62cb40e26746",
   "metadata": {
    "id": "c7826e48-8e1f-43d4-9899-4dc59cab7dbf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Urls are not essential for many analysis process, hence they need to be removed.\n",
    "- We can use regex to identify and remove the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6ed43-2e2b-452f-adfc-507713d81ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "03f6ed43-2e2b-452f-adfc-507713d81ee4",
    "outputId": "e142698b-ebba-47ca-b0c5-31d0c9f26c4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identifying and substituting urls using the pattern 'https\\S+' for urls\n",
    "text_clean = re.sub(r'http\\S+', '', ' '.join(text_clean), flags=re.MULTILINE)\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8656e60-beef-4c7a-b490-813722ef5a97",
   "metadata": {
    "id": "a0f7ef41-740c-4972-89f1-de238940fa2d"
   },
   "source": [
    "<a id = 'c'>\n",
    "<font size = 5 color = seagreen> <b> c. Remove punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e3da2-7b5d-485e-be9a-ac7b388eee42",
   "metadata": {
    "id": "a0f7ef41-740c-4972-89f1-de238940fa2d"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Punctuations are not always useful for anlaysis hence they shall also be removed.\n",
    "- The list of punctuations can be obtained from the `string` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fabf7a-c4d7-4db1-b9c0-0d7622400b6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "f1fabf7a-c4d7-4db1-b9c0-0d7622400b6c",
    "outputId": "aa18cdfd-d7b9-461b-c4b0-0990b46907cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_clean = [w for w in text_clean if w not in punctuation]\n",
    "print(f\"Cleaned  : \\n{''.join(text_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q3tqmRGTwY6m",
   "metadata": {
    "id": "Q3tqmRGTwY6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "\n",
    "**Note:**\n",
    "- In sentiment analysis sometimes punctuations like ! or ? may be significant for analysis.\n",
    "- Text cleaning steps should be customised based on the analysis objective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8974b-d0b7-4f6a-9508-ada717b371ab",
   "metadata": {
    "id": "c70685b6-e2d2-4260-8e95-24a51ef6c632"
   },
   "source": [
    "<a id = 'd'>\n",
    "<font size = 5 color = seagreen> <b>d. Remove emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc47e3-b499-409f-ba53-79135f2dce7a",
   "metadata": {
    "id": "c70685b6-e2d2-4260-8e95-24a51ef6c632"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Most of the text from the social media is nowadays filled with emoticons.\n",
    "- Handling emoticons becomes a necessary part of NLP pipeline.\n",
    "- They may be directly removed for simplicity.\n",
    "- This can also be achieved using regex by specifying the unicodes for these emoticons as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f3067-4ee0-4752-93d1-92284a5eb072",
   "metadata": {
    "id": "855f3067-4ee0-4752-93d1-92284a5eb072",
    "tags": []
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "def strip_emoji(text):\n",
    "    return RE_EMOJI.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b25294-da9b-48d8-ab6a-74d91bd9708a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "c4b25294-da9b-48d8-ab6a-74d91bd9708a",
    "outputId": "23e136a5-5fc7-4239-f619-ba016796a0e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use function to remove emoticons from text\n",
    "text_clean = strip_emoji(''.join(text_clean))\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DxssILQCzKdt",
   "metadata": {
    "id": "DxssILQCzKdt"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "\n",
    "**Note :**\n",
    " - Emoticons may be replaced with their intended meaning in form of text.\n",
    " - For example: üòÄ translates to  happy face.\n",
    " - This process is used in the vader sentiment package in data cleaning steps in sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbf2dd-65ce-4660-8870-757d52ee7690",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
